{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c4642e-cb22-49f1-803f-f8d66e99bed5",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "So far we have discussed k-armed bandit problems with stationary and non-stationary environments and greedy and $epsilon$-greedy agents. However, the k-armed bandit scenario that we implemented was not complex. The agent was always presented with the same situation, in front of k arms where each k was an independent action it could take. The scenario never really changed after each interaction and the only (slightly) complex implementation was building the ***non-stationary*** environment.\n",
    "\n",
    "MDPs were introduced in order to describe and handle more complex situations. A basic MDP framework consists of:\n",
    "\n",
    "* States\n",
    "* Actions\n",
    "* Rewards \n",
    "* Transitions \n",
    "\n",
    "### States\n",
    "State is described as the fundamental information needed to make a decision (to take an action). This is importan because in MDPs each state is an isolated case. States aren't constrained to physical locations or objects. There is no limit on the amount of information that a state can have. \n",
    "\n",
    "Here, we define new states based on what the agent chose in the previous interaction. We should also keep in mind that even though there is no real limit on the number of states an environment can have, the more teh states the harder it is for our agent to learn. *We aim at having a good balance between the information provided in each state and the number of states that the environment has*. \n",
    "\n",
    "### Actions\n",
    "Think of the actions as the way that our agent intercats with the environment. The action the agent can take depends on the state it finds itself, and every state can have a unique set of actions. Actions also define how we move from one state to another\n",
    "\n",
    "### Rewards \n",
    "Rewards are scalar values that the agent receives after taking an action. A reward can be thought of as a measurement of hoe good or bad the agent is doing in the environment. Every time we an action and (a path) we get a reward that is either positive, negative or zero. The o bjective of the agent is to maximise the reward values by taking the right actions at the right states. \n",
    "\n",
    "In RL, assigning rewards to an environment is one of the most difficult and crucial tasks. Since rewards shape the behaviour of the agent, any misalignement between the rewards can lead to unexpected behaviours.\n",
    "\n",
    "### Transitions\n",
    "Transition models represent the way the environment responds to the agent's interaction and determine the probabilities of going from one state to another.\n",
    "\n",
    "A transition model is usually represented as a function named $p$ and is often written like this:\n",
    "\n",
    "$$p(s',r|s,\\alpha)$$\n",
    "\n",
    "Here, $s'$ is the next state, $r$ is the reward, $s$ is the current state and $\\alpha$ is the action taken in the current state. The formula shwos the probability of reaching the next state ($s'$) and obtaining a reward ($r$), given that we are currently at some state ($s$) and have taken an action ($\\alpha$). \n",
    "\n",
    "We can think of the transition model as an accurate representation of the rules of the game.\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d27e7-ff78-4ccb-a7f9-563e68ab90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import spaces \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a254d-3721-4ab5-9e99-8c9643a8705c",
   "metadata": {},
   "source": [
    "We can now built our environment using the gym interface. \n",
    "\n",
    "#### the __init__() function\n",
    "\n",
    "In the initialisation function we add all the primary configurations that we need for the env to work properly. Most of the times we mainly need to define two required properties: the ```action space``` and the ```observation space```, which define the number of actions the agent may find and what it should expect to receive from the state of the environment. We use the ```.Discrete()``` method to define our states. For now we will create 2 states. State 1 will have 3 actions and state 2 will have 4 actions.\n",
    "\n",
    "#### step() function\n",
    "This is the function that our agent calls for each action within the environment.\n",
    "```step function``` is responsible for the evaluation of each action the reward each action comes with and transition to the next state. \n",
    "\n",
    "In a few words, depending on the agent's current position, the agent responds by taking an action  and that action generates the ```next_state``` and some ```reward```.  \n",
    "\n",
    "#### reset function\n",
    "This function resets the agent's position. Given that the ```reset function``` returns a state we are just reusing the ```next_state function```.\n",
    "\n",
    "#### render function\n",
    "Here we define how we want to visualise the env. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e537ff7-e4d3-4f8f-b09e-bd17c2c5fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainEnv(gym.Env):\n",
    "    '''\n",
    "    This class works with 5 functions: __init__, next_observation, step, reset, render.\n",
    "    We will create a new environment for our agent using the spaces module of the gym interface. \n",
    "    Thes spaces module is used to create both the states and the actions for each state.\n",
    "    '''\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(MainEnv, self).__init__()\n",
    "        \n",
    "        self.observation_space = spaces.Discrete(2) # define the states\n",
    "        self.action_space = spaces.Tuple((spaces.Discrete(3), spaces.Discrete(4))) # tuple with actions for each state\n",
    "        self.position = np.random.randint(2)\n",
    "        \n",
    "    def next_state(self):\n",
    "        return {'state': self.position}\n",
    "    \n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        INPUT: action (A)\n",
    "        We define the actions as:\n",
    "        STATE (S) 1 is a 3-armed bandit:\n",
    "        A1 = arm 1 -- goes to S 1 -- reward: 0.5\n",
    "        A2 = arm 2 -- goes to S 1 -- reward: -1\n",
    "        A3 = arm 3 -- goes to S 2 (70%) or goes to state 1 (30%) -- reward: (-2, -0.5)\n",
    "        \n",
    "        STATE (S) 2 is a 4-armed bandit:\n",
    "        A1 = arm 1 -- goes to S 2 -- reward: -2\n",
    "        A2 = arm 2 -- goes to S 2 -- reward: 1.2\n",
    "        A3 = arm 3 -- goes to S 2 -- reward: -1.5\n",
    "        A4 = arm 4 -- goes to S 1 (80%) or goes to S 2 (20%) -- reward: (-2, -0.5)\n",
    "        \n",
    "        OUTPUT:\n",
    "        next state (observation) = the new position of the agent \n",
    "        reward = the amount of reward for the action taken \n",
    "        done = whether the episode has ended \n",
    "        info = diagnositcs dict\n",
    "        '''\n",
    "        \n",
    "        transitions_1 = {\n",
    "            0: lambda: [0.5, 0],\n",
    "            1: lambda: [-1, 0],\n",
    "            2: lambda: [[-2, 1], [-0.5, 0]][np.random.choice(2, p=[0.7, 0.3])]\n",
    "        }\n",
    "        \n",
    "        transitions_2 = {\n",
    "            0: lambda: [-2, 1],\n",
    "            1: lambda: [1.2, 1],\n",
    "            2: lambda: [-1.5, 1],\n",
    "            3: lambda: [[-2, 0], [-0.5, 1]][np.random.choice(2, p=[0.8, 0.2])] \n",
    "        }\n",
    "        \n",
    "        reward = None\n",
    "        new_state = None \n",
    "        if (self.position == 0):\n",
    "            reward, new_state = transitions_1[action]() # agent is at state 1\n",
    "        else:\n",
    "            reward, new_state = transitions_2[action]() # agent is at state 2\n",
    "            \n",
    "        self.position = new_state # move the agent to the next state\n",
    "        return self.next_state(), reward, False, {}  \n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the agent's position\n",
    "        self.pisition = np.random.randint(2)\n",
    "        return self.next_state()\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            pretty_print_state = {\n",
    "                0: \"state 1\",\n",
    "                1: \"state 2\"\n",
    "            }\n",
    "            print('current state: {}'.format(print_state[self.position]))\n",
    "        else:\n",
    "            raise NotImplementedError()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7c5fa-b477-4ef8-8819-c3bfd8caee31",
   "metadata": {},
   "source": [
    "## Test the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fdd04-deee-4766-b928-29cfd5eccdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MainEnv()\n",
    "this_state = env.position\n",
    "steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e089ad1-84dd-4a5e-823e-77f79db11a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_a_0 = {\n",
    "    0: \"Arm 1\",\n",
    "    1: \"Arm 2\",\n",
    "    2: \"Arm 3\"\n",
    "}\n",
    "\n",
    "print_a_1 = {\n",
    "    0: \"Arm 1\",\n",
    "    1: \"Arm 2\",\n",
    "    2: \"Arm 3\",\n",
    "    3: \"Arm 4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ff583-6df4-4363-b835-d38b917c941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(steps):\n",
    "    env.render()\n",
    "    a = env.action_space[this_state].sample() # choose an action from the current state\n",
    "    print_action = None\n",
    "    \n",
    "    if this_state == 0:\n",
    "        print_action = pretty_print_a_0[a]\n",
    "    else:\n",
    "        print_action = pretty_print_a_1[a]\n",
    "        \n",
    "    print('action taken: {}'.format(print_action))\n",
    "    s_prime, reward, _,_ = env.step(a) # execute a step, get reward and move to the next state\n",
    "    this_state = s_prime['state']\n",
    "    print('reward obtained: {}'.format(reward))\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b388bf-369b-423f-b937-322a12e4c895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131eaea-8947-466c-9fb4-17c1175be2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
