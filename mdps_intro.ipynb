{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c4642e-cb22-49f1-803f-f8d66e99bed5",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "So far we have discussed k-armed bandit problems with stationary and non-stationary environments and greedy and $epsilon$-greedy agents. However, the k-armed bandit scenario that we implemented was not complex. The agent was always presented with the same situation, in front of k arms where each k was an independent action it could take. The scenario never really changed after each interaction and the only (slightly) complex implementation was building the ***non-stationary*** environment.\n",
    "\n",
    "MDPs were introduced in order to describe and handle more complex situations. A basic MDP framework consists of:\n",
    "\n",
    "* States\n",
    "* Actions\n",
    "* Rewards \n",
    "* Transitions \n",
    "\n",
    "### States\n",
    "State is described as the fundamental information needed to make a decision (to take an action). This is importan because in MDPs each state is an isolated case. States aren't constrained to physical locations or objects. There is no limit on the amount of information that a state can have. \n",
    "\n",
    "Here, we define new states based on what the agent chose in the previous interaction. We should also keep in mind that even though there is no real limit on the number of states an environment can have, the more teh states the harder it is for our agent to learn. *We aim at having a good balance between the information provided in each state and the number of states that the environment has*. \n",
    "\n",
    "### Actions\n",
    "Think of the actions as the way that our agent intercats with the environment. The action the agent can take depends on the state it finds itself, and every state can have a unique set of actions. Actions also define how we move from one state to another\n",
    "\n",
    "### Rewards \n",
    "Rewards are scalar values that the agent receives after taking an action. A reward can be thought of as a measurement of hoe good or bad the agent is doing in the environment. Every time we an action and (a path) we get a reward that is either positive, negative or zero. The o bjective of the agent is to maximise the reward values by taking the right actions at the right states. \n",
    "\n",
    "In RL, assigning rewards to an environment is one of the most difficult and crucial tasks. Since rewards shape the behaviour of the agent, any misalignement between the rewards can lead to unexpected behaviours.\n",
    "\n",
    "### Transitions\n",
    "Transition models represent the way the environment responds to the agent's interaction and determine the probabilities of going from one state to another.\n",
    "\n",
    "A transition model is usually represented as a function named $p$ and is often written like this:\n",
    "\n",
    "$$p(s',r|s,\\alpha)$$\n",
    "\n",
    "Here, $s'$ is the next state, $r$ is the reward, $s$ is the current state and $\\alpha$ is the action taken in the current state. The formula shwos the probability of reaching the next state ($s'$) and obtaining a reward ($r$), given that we are currently at some state ($s$) and have taken an action ($\\alpha$). \n",
    "\n",
    "We can think of the transition model as an accurate representation of the rules of the game.\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128d27e7-ff78-4ccb-a7f9-563e68ab90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import spaces \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a254d-3721-4ab5-9e99-8c9643a8705c",
   "metadata": {},
   "source": [
    "We can now built our environment using the gym interface. \n",
    "\n",
    "#### the __init__() function\n",
    "\n",
    "In the initialisation function we add all the primary configurations that we need for the env to work properly. Most of the times we mainly need to define two required properties: the ```action space``` and the ```observation space```, which define the number of actions the agent may find and what it should expect to receive from the state of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e537ff7-e4d3-4f8f-b09e-bd17c2c5fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainEnv(gym.Env):\n",
    "    '''\n",
    "    This class works with 5 functions: __init__, next_observation, step, reset, render.'''\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
