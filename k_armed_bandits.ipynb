{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2793947-0f3f-44dc-83c4-5c3d1f748a1d",
   "metadata": {},
   "source": [
    "# K-Armed Bandit Problems \n",
    "\n",
    "First notebook of the series on Reinforcement Learning (RL). This notebook (along with the rest of the reinforcement learning series) is based on [this introductory book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) by R.S. Sutton and A.G Barto and on this [coursera cource](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome). The reason I created these notebooks is mainly to gain a better understanding of RL myself and to try and implement the theory and models with python.\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68663c7-bb04-407a-9da6-5c2e9782ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm \n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d97c71-98f5-4cc0-b731-8927195d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d49c-65a5-4b97-9cfb-bf690d60d3b6",
   "metadata": {},
   "source": [
    "## What are the K-Armed Bandits?\n",
    "\n",
    "In K-Armed Bandits there is one ***state** and k-**actions** for an **agent** to interact within an **environment***. Every action provides a (random) reward with an unknown value. The goal of the agent is to maximise the reward in the long run and to do so, the agent needs to explore all the actions and find the one that generates the highest reward. \n",
    "\n",
    "Based on the definition above, I will start building the structure of the environment using the **OpenAI**'s gym interface. The blocks of code below are highly influenced by [this code](https://github.com/diegoalejogm/openai-k-armed-bandits/tree/master/gym_armed_bandits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1a7d22-2091-4dff-bf7a-6b0237defce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[5, 1, 0, -10]]) # means for a 4-armed bandit \n",
    "sd = np.array([[1, 0.1, 5, 1]]) # standard deviations for a 4-armed bandit \n",
    "bandit_len = len(m[0]) # number of bandits \n",
    "# a = np.array([[1]]) # actions for a 4-armed bandit, play around with indecies: 0-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bf44e-67a3-4d7d-bee1-cd540c5156ee",
   "metadata": {},
   "source": [
    "## Building the structure of the environment \n",
    "\n",
    "The environment will receive ```np.array()``` for means and standard deviations for each action with dimensions ```num_experiments x num_bandits```. For the 4-armed bandit the action indecies should be from 0-3 (one at a time). \n",
    "\n",
    "### sample from the specified bandit and take steps. \n",
    "\n",
    "The size of the vector for each step is of size ```num_experiments```, where the action to take is specified for each experiment. As a return we get ```r``` which is the reward for each experiment/action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe6ceb4-0821-46cc-99b5-061cfb8e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditsEnv():\n",
    "    \n",
    "    '''Here, we get an array of length k. Each item is a function which samples \n",
    "    from a specified distribution.'''\n",
    "    \n",
    "    def __init__(self, m, sd):\n",
    "        \n",
    "        assert len(m.shape) == 2\n",
    "        assert len(sd.shape) == 2\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # define action and obs spaces\n",
    "        self.mean = m\n",
    "        self.sd = sd\n",
    "        self.nb_bandits = len(m[0]) # number of bandits \n",
    "        self.nb_exp = len(m) # number of experiments \n",
    "        self.action_space = spaces.Discrete(self.nb_bandits) # define action space\n",
    "        self.obs_space = spaces.Discrete(1) # define observetional space\n",
    "    \n",
    "    # define the step function  \n",
    "    def step(self, a):\n",
    "        \n",
    "        # input arg: array that contains the current action \n",
    "        # output: reward array for the current action \n",
    "        \n",
    "        assert (a < self.nb_bandits).all()\n",
    "        \n",
    "        sampled_m = self.mean[np.arange(self.nb_exp), a]\n",
    "        sampled_sd = self.sd[np.arange(self.nb_exp), a]\n",
    "        r = np.random.normal(loc=sampled_m, scale=sampled_sd, size=(self.nb_exp, self.nb_exp))\n",
    "        \n",
    "        # for now return only reward \n",
    "        return r      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b092d-bade-4dc4-a97f-9c8fba388aff",
   "metadata": {},
   "source": [
    "### Creating and interacting with the environment \n",
    "Now let's create an environment by passing arrays of the means and standard deviations as input arguments. Every time we interact with the environment we specify which bandid we want to pull, and the environment will return the associated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794a5d9d-d73c-4365-a348-fb66a5b05405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward for bandit 1 was: [5.44301164]\n",
      " reward for bandit 2 was: [1.07964811]\n",
      " reward for bandit 3 was: [-2.94635716]\n",
      " reward for bandit 4 was: [-10.10814296]\n"
     ]
    }
   ],
   "source": [
    "# create an environment \n",
    "test_env = BanditsEnv(m, sd)\n",
    "rewards = np.zeros(m.shape) # we'll store the rewards here\n",
    "\n",
    "# get the reward for each action\n",
    "for i in range(4):\n",
    "    a = np.array([[i]])\n",
    "    r = test_env.step(a)\n",
    "    rewards[0,i] = r\n",
    "    print(\" reward for bandit\", i+1, \"was:\", r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98dd3b-e192-4e31-a356-e94bc75819fa",
   "metadata": {},
   "source": [
    "Above I created 4 bandits (with their means and sd). I interacted with each bandit to see the reward it returns. The reward looks random, however it seems that it falls near the mean of each bandit. The sd shows how far the rewards will fall for each bandit. For example, the deviation for the second bandit is very small (0.1) and we see that the reward is very close to the mean. The sd of the third bandit is high (5) and it seems that the reward always falls quit far from the mean.\n",
    "\n",
    "#### Exercise and Practice \n",
    "This example has 4 bandits. Play around with the Environment and try running it with more bandits (e.g. 6 or 10) or change the means and standard deviations.\n",
    "\n",
    "### The exploration - Explotation Dilemma\n",
    "Note that interacting with each bandit just once doesn't tell us much. The goal in such tasks is to interact with the environment many times, explore all actions/bandits and build a distribution of rewards for each action/bandit, in an attempt to choose an action that gives the highest reward.\n",
    "\n",
    "This notion is very familiar in situations from real life. Imagine being at a restaurant and trying to choose between ordering a meal that you have tried before and you know is good (previous high reward) or ordering a meal that is new but looks delicious. What should you choose? You know that the familiar meal will give you a high reward, but what if the new meal is even tastier? \n",
    "\n",
    "Choosing the familiar meal is a greedy behaviour because you already know that this choice will give you high reward (exploitation), however, it stops you from trying other options (exploration). This is called the exploration-explotation dilemma and is a very common problem in k-armed bandit tasks. The dilemma is presented every time the agent is in an **uknown or partially known** environment and trys to optimise its interaction with the environment. The agent wants to get the highest possible reward but does not know which action is the best for achieving this goal. By exploring (trying different actions or trying different meals every time we go to a specific restaurant), we interact with the environment, we learn more about it but we also sacrifice the chance of getting a known (high reward). This may look sub-optimal, but, **in the long run** new actions may give higher rewards than we initially thought.\n",
    "\n",
    "This dilemma is very common in RL situations, thus developing good strategies for dealing with it is very important.\n",
    "\n",
    "Before developing the strategies we should first see how we can evaluate our actions. \n",
    "\n",
    "### Evaluating actions\n",
    "\n",
    "Now that we have developed our environment, let us evaluate our actions. For each of the 4 arms/actions we need a value of what we expect to get from the given action. In the beginning, we do not know what to expect, however, every time we interact with the environment (every time we take an action) we update our expectations.\n",
    "\n",
    "I will start by interacting with the environment 5 times. And during each interaction, I will be storing the first action reward values to a list. For this behaviour, I will receive a list of rewards ```first_a```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446fc591-bbb9-4cee-b8bf-c2ec3313d265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.116602828479659,\n",
       " 5.218330012671211,\n",
       " 5.239502090146444,\n",
       " 4.028523814943232,\n",
       " 5.536167067194524]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = BanditsEnv(m, sd)\n",
    "first_a = []\n",
    "s = 4 # nubmer of bandit arms/actions to take\n",
    "n = 5 # number of steps (times we interact with the environment)\n",
    "\n",
    "# get the reward for each action n times (n pulls) and store the reward values for the first action in a list\n",
    "for i in range (n):\n",
    "    rewards = np.zeros(m.shape) # for every pull, we'll store the rewards here\n",
    "    \n",
    "    for j in range(s):\n",
    "        a = np.array([[j]])\n",
    "        r = eval_env.step(a)\n",
    "        rewards[0,j] = r\n",
    "        \n",
    "    first_a.append(rewards[0,0]) # store the actions for the 1st action/arm\n",
    "\n",
    "first_a # visualise     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25814f3-482c-4f4e-ba6b-6a057a233947",
   "metadata": {},
   "source": [
    "The expected value for the first action is the average of all values of that action. This is how we calculate the expected value for a given action: \n",
    "\n",
    "\n",
    "$$Q_{n}\\dot{=}\\frac{R_{1}+R_{2}+...+R_{n}}{n}$$\n",
    "\n",
    "So, now that we have the formula, let us calculate the expected reward value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4f5e8b-f7a9-4030-acaa-d4f45ff09ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027825162687014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first = sum(first_a)/len(first_a)\n",
    "Q_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5528b62-6c07-4352-b644-39bb6b6e96b1",
   "metadata": {},
   "source": [
    "We could also write the above code as: ```Q_first = np.asarray(first_a).mean()```\n",
    "\n",
    "If we pull the arm/action again (n + 1 times) we would need to average again, and if we pull it one more time we average again... this can become quite unpractical and the way to handle it is by adding an **update rule** into our formula to get a new combined average:\n",
    "\n",
    "$$ n = n + 1 $$\n",
    "\n",
    "$$Q_{n} = Q_{n+1}$$\n",
    "\n",
    "$$Q_{n}\\dot{=}Q_{n-1}+\\frac{1}{n} [R_{n}-Q_{n-1}]$$\n",
    "\n",
    "We do this for every action and we update the formula for each step we add. If we play around with the step size $\\frac{1}{n}$ we will notice that this update rule starts converging as we increase the step size and this occurs because as n gets larger the value changes less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09633ff-e956-4cc0-b0c0-573b610bb161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward for bandit 1 was: [5.40592744]\n",
      " reward for bandit 2 was: [1.02854473]\n",
      " reward for bandit 3 was: [2.30633808]\n",
      " reward for bandit 4 was: [-9.52412903]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.116602828479659,\n",
       " 5.218330012671211,\n",
       " 5.239502090146444,\n",
       " 4.028523814943232,\n",
       " 5.536167067194524,\n",
       " 5.405927437557384]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = np.zeros(m.shape) # for every pull, we'll store the rewards here\n",
    "\n",
    "for j in range(s):\n",
    "    a = np.array([[j]])\n",
    "    r = eval_env.step(a)\n",
    "    rewards[0,j] = r\n",
    "    print(\" reward for bandit\", j+1, \"was:\", r[0])\n",
    "\n",
    "first_a.append(rewards[0,0]) \n",
    "first_a # visualise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6d6984-cf5d-4d29-a312-a8b9b395561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.090842208498742"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first = sum(first_a)/len(first_a)\n",
    "Q_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a943deb-cd88-45d0-a730-7c7cc6bf21e4",
   "metadata": {},
   "source": [
    "This is a simple way of incrementing steps and adding reward values. To (manually) visualise the convergence run the above to blocks of code a few times until ```Q_first``` srarts decreasing.\n",
    "\n",
    "**Note**: To make our code cleaner and easier to read we can add the incremental part in a function. This function will do exactly what the last formula (with the updating rule does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a36b061-9d7c-4797-b5ca-72dc329da252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_rule(aval, m, n):\n",
    "    \n",
    "    # aval = (previous) average value\n",
    "    # m = np array with means of k-arms\n",
    "    # n = step size\n",
    "    \n",
    "    n = n + 1 # increment step size by 1\n",
    "    rewards = np.zeros(m.shape)\n",
    "    \n",
    "    for j in range(s):\n",
    "        a = np.array([[j]])\n",
    "        r = eval_env.step(a)\n",
    "        rewards[0,j] = r\n",
    "        \n",
    "    temp = rewards[0,0]\n",
    "    upd_aver = aval + 1 / n * (temp - aval)\n",
    "    return upd_aver, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3d99884-fd54-42ca-9788-a5b69e5f32bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.91210560832286"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first, n = incremental_rule(Q_first, m, n)   \n",
    "Q_first "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b6545-667d-4e04-aba5-d3603e8d2cdf",
   "metadata": {},
   "source": [
    "## Exploitation - The Greedy Agent \n",
    "\n",
    "As mentioned earlier, there are two startegies for choosing the best action each time. The agent can either exploit or explore actions and their rewards. \n",
    "\n",
    "I will focus on exploitation for now. In k-armed badint problems the optimal behaviour is that the agent chooses the action that gives the best reward in the long run. The agent needs to interact with the environment in order to learn. However, there might be times that the agent is **greedy**, that is, he does not choose to explore all actions and reward values, but chooses the current best value every time. Think of this behaviour like going to a nice restaurant and choosing one specific meal every single time, because you ordered it once and you liked it. But this kind of behaviour does not let you explore other choises. \n",
    "\n",
    "This is exploitation and when an agent chooses greedy actions we use the ```argmax()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e9a79-3bd8-4842-9959-763f00d3426a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e92220-d421-40db-9872-751cca5e87ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
