{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2793947-0f3f-44dc-83c4-5c3d1f748a1d",
   "metadata": {},
   "source": [
    "# K-Armed Bandit Problems \n",
    "\n",
    "First notebook of the series on Reinforcement Learning (RL). This notebook (along with the rest of the reinforcement learning series) is based on [this introductory book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) by R.S. Sutton and A.G Barto and on this [coursera cource](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome). The reason I created these notebooks is mainly to gain a better understanding of RL myself and to try and implement the theory and models with python.\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68663c7-bb04-407a-9da6-5c2e9782ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm \n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d97c71-98f5-4cc0-b731-8927195d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d49c-65a5-4b97-9cfb-bf690d60d3b6",
   "metadata": {},
   "source": [
    "## What are the K-Armed Bandits?\n",
    "\n",
    "In K-Armed Bandits there is one ***state** and k-**actions** for an **agent** to interact within an **environment***. Ecery action provides a (random) reward with an unknown value. The goal of the agent is to maximise the reward in the long run and to do so, the agent needs to explore all the actions and find the one that generates the highest reward. \n",
    "\n",
    "Based on the definition above, I will start building the structure of the environment using the **OpenAI**'s gym interface. The blocks of code below are highly influenced by [this code](https://github.com/diegoalejogm/openai-k-armed-bandits/tree/master/gym_armed_bandits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1a7d22-2091-4dff-bf7a-6b0237defce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[5, 1, 0, -10]]) # means for a 4-armed bandit \n",
    "sd = np.array([[1, 0.1, 5, 1]]) # standard deviations for a 4-armed bandit \n",
    "bandit_len = len(m[0]) # number of bandits \n",
    "# a = np.array([[1]]) # actions for a 4-armed bandit, play around with indecies: 0-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bf44e-67a3-4d7d-bee1-cd540c5156ee",
   "metadata": {},
   "source": [
    "## Building the structure of the environment \n",
    "\n",
    "The environment will receive ```np.array()``` for means and standard deviations for each action with dimensions ```num_experiments x num_bandits```. For the 4-armed bandit the action indecies should be from 0-3 (one at a time). \n",
    "\n",
    "### sample from the specified bandit and take steps. \n",
    "\n",
    "The size of the vector for each step is of size ```num_experiments```, where the action to take is specified for each experiment. As a return we get ```r``` which is the reward for each experiment/action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfe6ceb4-0821-46cc-99b5-061cfb8e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditsEnv():\n",
    "    \n",
    "    '''Here, we get an array of length k. Each item is a function which samples \n",
    "    from a specified distribution.'''\n",
    "    \n",
    "    def __init__(self, m, sd):\n",
    "        \n",
    "        assert len(m.shape) == 2\n",
    "        assert len(sd.shape) == 2\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # define action and obs spaces\n",
    "        self.mean = m\n",
    "        self.sd = sd\n",
    "        self.nb_bandits = len(m[0]) # number of bandits \n",
    "        self.nb_exp = len(m) # number of experiments \n",
    "        self.action_space = spaces.Discrete(self.nb_bandits) # define action space\n",
    "        self.obs_space = spaces.Discrete(1) # define observetional space\n",
    "    \n",
    "    # define the step function  \n",
    "    def step(self, a):\n",
    "        \n",
    "        # input arg: array that contains the current action \n",
    "        # output: reward array for the current action \n",
    "        \n",
    "        assert (a < self.nb_bandits).all()\n",
    "        \n",
    "        sampled_m = self.mean[np.arange(self.nb_exp), a]\n",
    "        sampled_sd = self.sd[np.arange(self.nb_exp), a]\n",
    "        r = np.random.normal(loc=sampled_m, scale=sampled_sd, size=(self.nb_exp, self.nb_exp))\n",
    "        \n",
    "        # for now return only reward \n",
    "        return r      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b092d-bade-4dc4-a97f-9c8fba388aff",
   "metadata": {},
   "source": [
    "### Creating and interacting with the environment \n",
    "Now let's create an environment by passing arrays of the means and standard deviations as input arguments. Every time we interact with the environment we specify which bandid we want to pull, and the environment will return the associated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "794a5d9d-d73c-4365-a348-fb66a5b05405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward for bandit 1 was: [3.3710695]\n",
      " reward for bandit 2 was: [1.04975842]\n",
      " reward for bandit 3 was: [3.61570152]\n",
      " reward for bandit 4 was: [-10.9507265]\n"
     ]
    }
   ],
   "source": [
    "# create an environment \n",
    "test_env = BanditsEnv(m, sd)\n",
    "rewards = np.zeros(m.shape) # we'll store the rewards here\n",
    "\n",
    "# get the reward for each action\n",
    "for i in range(4):\n",
    "    a = np.array([[i]])\n",
    "    r = test_env.step(a)\n",
    "    rewards[0,i] = r\n",
    "    print(\" reward for bandit\", i+1, \"was:\", r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98dd3b-e192-4e31-a356-e94bc75819fa",
   "metadata": {},
   "source": [
    "Above I created 4 bandits (with their means and sd). I interacted with each bandit to see the reward it returns. The reward looks random, however it seems that it falls near the mean of each bandit. The sd shows how far the rewards will fall for each bandit. For example, the deviation for the second bandit is very small (0.1) and we see that the reward is very close to the mean. The sd of the third bandit is high (5) and it seems that the reward always falls quit far from the mean.\n",
    "\n",
    "#### Exercise and Practice \n",
    "This example has 4 bandits. Play around with the Environment and try running it with more bandits (e.g. 6 or 10) or change the means and standard deviations.\n",
    "\n",
    "### The exploration - Explotation Dilemma\n",
    "Note that interacting with each bandit just once doesn't tell us much. The goal in such tasks is to interact with the environment many times, explore all actions/bandits and build a distribution of rewards for each action/bandit, in an attempt to choose an action that gives the highest reward.\n",
    "\n",
    "This notion is very familiar in situations from real life. Imagine being at a restaurant and trying to choose between ordering a meal that you have tried before and you know is good (previous high reward) or ordering a meal that is new but looks delicious. What should you choose? You know that the familiar meal will give you a high reward, but what if the new meal is even tastier? \n",
    "\n",
    "Choosing the familiar meal is a greedy behaviour because you already know that this choice will give you high reward (exploitation), however, it stops you from trying other options (exploration). This is called the exploration-explotation dilemma and is a very common problem in k-armed bandit tasks. The dilemma is presented every time the agent is in an **uknown or partially known** environment and trys to optimise its interaction with the environment. The agent wants to get the highest possible reward but does not know which action is the best for achieving this goal. By exploring (trying different actions or trying different meals every time we go to a specific restaurant), we interact with the environment, we learn more about it but we also sacrifice the chance of getting a known (high reward). This may look sub-optimal, but, **in the long run** new actions may give higher rewards than we initially thought.\n",
    "\n",
    "This dilemma is very common in RL situations, thus developing good strategies for dealing with it is very important.\n",
    "\n",
    "Before developing the strategies we should first see how we can evaluate our actions. \n",
    "\n",
    "### Evaluating actions\n",
    "\n",
    "Now that we have developed our environment, let us evaluate our actions. For each of the 4 arms we need a value of what we expect to get from the given action. In the beginning, we do not know what to expect, however, every time we interact with the environment (every time we take an action) we update our expectations.\n",
    "\n",
    "I will start by pulling the first arm 5 times. For this behaviour, I will receive a list of rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "446fc591-bbb9-4cee-b8bf-c2ec3313d265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.2614877253822656,\n",
       " 4.451492078087157,\n",
       " 3.556804178285624,\n",
       " 4.408528829420389,\n",
       " 5.958797910455155]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = BanditsEnv(m, sd)\n",
    "first_arm = []\n",
    "\n",
    "# get the reward for each action 5 times (5 pulls) and store the values for the first arm in a list\n",
    "for i in range (5):\n",
    "    rewards = np.zeros(m.shape) # for every pull, we'll store the rewards here\n",
    "    \n",
    "    for j in range(4):\n",
    "        a = np.array([[j]])\n",
    "        r = eval_env.step(a)\n",
    "        rewards[0,j] = r\n",
    "        \n",
    "    first_arm.append(rewards[0,0]) # store the actions for the 1st arm\n",
    "\n",
    "first_arm # visualise     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad510c-699b-49cc-a5a1-f102a992036c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f5e8b-f7a9-4030-acaa-d4f45ff09ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
