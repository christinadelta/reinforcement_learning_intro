{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2793947-0f3f-44dc-83c4-5c3d1f748a1d",
   "metadata": {},
   "source": [
    "# K-Armed Bandit Problems \n",
    "\n",
    "First notebook of the series on Reinforcement Learning (RL). This notebook (along with the rest of the reinforcement learning series) is based on [this introductory book](http://incompleteideas.net/book/bookdraft2018jan1.pdf) by R.S. Sutton and A.G Barto and on this [coursera cource](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome). The reason I created these notebooks is mainly to gain a better understanding of RL myself and to try and implement the theory and models with python.\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68663c7-bb04-407a-9da6-5c2e9782ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm \n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d97c71-98f5-4cc0-b731-8927195d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d49c-65a5-4b97-9cfb-bf690d60d3b6",
   "metadata": {},
   "source": [
    "## What are the K-Armed Bandits?\n",
    "\n",
    "In K-Armed Bandits there is one ***state** and k-**actions** for an **agent** to interact within an **environment***. Every action provides a (random) reward with an unknown value. The goal of the agent is to maximise the reward in the long run and to do so, the agent needs to explore all the actions and find the one that generates the highest reward. \n",
    "\n",
    "Based on the definition above, I will start building the structure of the environment using the **OpenAI**'s gym interface. The blocks of code below are highly influenced by [this code](https://github.com/diegoalejogm/openai-k-armed-bandits/tree/master/gym_armed_bandits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1a7d22-2091-4dff-bf7a-6b0237defce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[5, 1, 0, -10]]) # means for a 4-armed bandit \n",
    "sd = np.array([[1, 0.1, 5, 1]]) # standard deviations for a 4-armed bandit \n",
    "bandit_len = len(m[0]) # number of bandits \n",
    "# a = np.array([[1]]) # actions for a 4-armed bandit, play around with indecies: 0-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bf44e-67a3-4d7d-bee1-cd540c5156ee",
   "metadata": {},
   "source": [
    "## Building the structure of the environment \n",
    "\n",
    "The environment will receive ```np.array()``` for means and standard deviations for each action with dimensions ```num_experiments x num_bandits```. For the 4-armed bandit the action indecies should be from 0-3 (one at a time). \n",
    "\n",
    "### sample from the specified bandit and take steps. \n",
    "\n",
    "The size of the vector for each step is of size ```num_experiments```, where the action to take is specified for each experiment. As a return we get ```r``` which is the reward for each experiment/action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe6ceb4-0821-46cc-99b5-061cfb8e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditsEnv():\n",
    "    \n",
    "    '''Here, we get an array of length k. Each item is a function which samples \n",
    "    from a specified distribution.'''\n",
    "    \n",
    "    metadata = {'render.mode': ['human']}\n",
    "    \n",
    "    def __init__(self, m, sd, exp):\n",
    "        \n",
    "        assert len(m.shape) == 2\n",
    "        assert len(sd.shape) == 2\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # define action and obs spaces\n",
    "        self.mean = m\n",
    "        self.sd = sd\n",
    "        self.nb_bandits = len(m[0]) # number of bandits \n",
    "        self.nb_exp = exp # number of experiments \n",
    "        self.action_space = spaces.Discrete(self.nb_bandits) # define action space\n",
    "        self.obs_space = spaces.Discrete(1) # define observetional space\n",
    "    \n",
    "    # define the step function  \n",
    "    def step(self, a):\n",
    "        \n",
    "        # input arg: array that contains the current action \n",
    "        # output: reward array for the current action \n",
    "        \n",
    "        assert (a < self.nb_bandits).all()\n",
    "        # assert self.action_space.contains(a) # assert action is valid\n",
    "        \n",
    "        sampled_m = self.mean[np.arange(self.nb_exp), a]\n",
    "        sampled_sd = self.sd[np.arange(self.nb_exp), a]\n",
    "        \n",
    "        if self.nb_exp == 1: # if we only have one experiment \n",
    "            r = np.random.normal(loc=sampled_m, scale=sampled_sd, size=(self.nb_exp, self.nb_exp,))\n",
    "        else:\n",
    "            r = np.random.normal(loc=sampled_m, scale=sampled_sd, size=(self.nb_exp, ))\n",
    "        \n",
    "        # for now return only reward \n",
    "        return r     \n",
    "    \n",
    "    def reset(self):\n",
    "        # is reseting a desired behaviour?\n",
    "        return 0\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        \n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b092d-bade-4dc4-a97f-9c8fba388aff",
   "metadata": {},
   "source": [
    "### Creating and interacting with the environment \n",
    "Now let's create an environment by passing arrays of the means and standard deviations as input arguments. Every time we interact with the environment we specify which bandid we want to pull, and the environment will return the associated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794a5d9d-d73c-4365-a348-fb66a5b05405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward for bandit 1 was: [5.41838396]\n",
      " reward for bandit 2 was: [1.23650429]\n",
      " reward for bandit 3 was: [-9.14185761]\n",
      " reward for bandit 4 was: [-10.79022285]\n"
     ]
    }
   ],
   "source": [
    "# create an environment \n",
    "n_exp = 1\n",
    "test_env = BanditsEnv(m, sd, n_exp)\n",
    "rewards = np.zeros(m.shape) # we'll store the rewards here\n",
    "\n",
    "# get the reward for each action\n",
    "for i in range(4):\n",
    "    a = np.array([[i]])\n",
    "    r = test_env.step(a)\n",
    "    rewards[0,i] = r\n",
    "    print(\" reward for bandit\", i+1, \"was:\", r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98dd3b-e192-4e31-a356-e94bc75819fa",
   "metadata": {},
   "source": [
    "Above I created 4 bandits (with their means and sd). I interacted with each bandit to see the reward it returns. The reward looks random, however it seems that it falls near the mean of each bandit. The sd shows how far the rewards will fall for each bandit. For example, the deviation for the second bandit is very small (0.1) and we see that the reward is very close to the mean. The sd of the third bandit is high (5) and it seems that the reward always falls quit far from the mean.\n",
    "\n",
    "#### Exercise and Practice \n",
    "This example has 4 bandits. Play around with the Environment and try running it with more bandits (e.g. 6 or 10) or change the means and standard deviations.\n",
    "\n",
    "### The exploration - Explotation Dilemma\n",
    "Note that interacting with each bandit just once doesn't tell us much. The goal in such tasks is to interact with the environment many times, explore all actions/bandits and build a distribution of rewards for each action/bandit, in an attempt to choose an action that gives the highest reward.\n",
    "\n",
    "This notion is very familiar in situations from real life. Imagine being at a restaurant and trying to choose between ordering a meal that you have tried before and you know is good (previous high reward) or ordering a meal that is new but looks delicious. What should you choose? You know that the familiar meal will give you a high reward, but what if the new meal is even tastier? \n",
    "\n",
    "Choosing the familiar meal is a greedy behaviour because you already know that this choice will give you high reward (exploitation), however, it stops you from trying other options (exploration). This is called the exploration-explotation dilemma and is a very common problem in k-armed bandit tasks. The dilemma is presented every time the agent is in an **uknown or partially known** environment and trys to optimise its interaction with the environment. The agent wants to get the highest possible reward but does not know which action is the best for achieving this goal. By exploring (trying different actions or trying different meals every time we go to a specific restaurant), we interact with the environment, we learn more about it but we also sacrifice the chance of getting a known (high reward). This may look sub-optimal, but, **in the long run** new actions may give higher rewards than we initially thought.\n",
    "\n",
    "This dilemma is very common in RL situations, thus developing good strategies for dealing with it is very important.\n",
    "\n",
    "Before developing the strategies we should first see how we can evaluate our actions. \n",
    "\n",
    "### Evaluating actions\n",
    "\n",
    "Now that we have developed our environment, let us evaluate our actions. For each of the 4 arms/actions we need a value of what we expect to get from the given action. In the beginning, we do not know what to expect, however, every time we interact with the environment (every time we take an action) we update our expectations.\n",
    "\n",
    "I will start by interacting with the environment 5 times. And during each interaction, I will be storing the first action reward values to a list. For this behaviour, I will receive a list of rewards ```first_a```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446fc591-bbb9-4cee-b8bf-c2ec3313d265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.918764465594603,\n",
       " 3.9343391969413,\n",
       " 4.344101558270384,\n",
       " 5.977984969903488,\n",
       " 3.684090174855558]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = BanditsEnv(m, sd, n_exp)\n",
    "first_a = []\n",
    "s = 4 # nubmer of bandit arms/actions to take\n",
    "n = 5 # number of steps (times we interact with the environment)\n",
    "\n",
    "# get the reward for each action n times (n pulls) and store the reward values for the first action in a list\n",
    "for i in range (n):\n",
    "    rewards = np.zeros(m.shape) # for every pull, we'll store the rewards here\n",
    "    \n",
    "    for j in range(s):\n",
    "        a = np.array([[j]])\n",
    "        r = eval_env.step(a)\n",
    "        rewards[0,j] = r\n",
    "        \n",
    "    first_a.append(rewards[0,0]) # store the actions for the 1st action/arm\n",
    "\n",
    "first_a # visualise     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25814f3-482c-4f4e-ba6b-6a057a233947",
   "metadata": {},
   "source": [
    "The expected value for the first action is the average of all values of that action. This is how we calculate the expected value for a given action: \n",
    "\n",
    "\n",
    "$$Q_{n}\\dot{=}\\frac{R_{1}+R_{2}+...+R_{n}}{n}$$\n",
    "\n",
    "So, now that we have the formula, let us calculate the expected reward value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4f5e8b-f7a9-4030-acaa-d4f45ff09ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.571856073113067"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first = sum(first_a)/len(first_a)\n",
    "Q_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5528b62-6c07-4352-b644-39bb6b6e96b1",
   "metadata": {},
   "source": [
    "We could also write the above code as: ```Q_first = np.asarray(first_a).mean()```\n",
    "\n",
    "If we pull the arm/action again (n + 1 times) we would need to average again, and if we pull it one more time we average again... this can become quite unpractical and the way to handle it is by adding an **update rule** into our formula to get a new combined average:\n",
    "$$n = n + 1$$\n",
    "\n",
    "$$Q_{n} = Q_{n+1}$$\n",
    "\n",
    "$$Q_{n}\\dot{=}Q_{n-1}+\\frac{1}{n} [R_{n}-Q_{n-1}]$$\n",
    "\n",
    "We do this for every action and we update the formula for each step we add. If we play around with the step size $\\frac{1}{n}$ we will notice that this update rule starts converging as we increase the step size and this occurs because as n gets larger the value changes less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09633ff-e956-4cc0-b0c0-573b610bb161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reward for bandit 1 was: [5.88548509]\n",
      " reward for bandit 2 was: [0.87262769]\n",
      " reward for bandit 3 was: [6.04797256]\n",
      " reward for bandit 4 was: [-8.94260892]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.918764465594603,\n",
       " 3.9343391969413,\n",
       " 4.344101558270384,\n",
       " 5.977984969903488,\n",
       " 3.684090174855558,\n",
       " 5.88548508504916]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = np.zeros(m.shape) # for every pull, we'll store the rewards here\n",
    "\n",
    "for j in range(s):\n",
    "    a = np.array([[j]])\n",
    "    r = eval_env.step(a)\n",
    "    rewards[0,j] = r\n",
    "    print(\" reward for bandit\", j+1, \"was:\", r[0])\n",
    "\n",
    "first_a.append(rewards[0,0]) \n",
    "first_a # visualise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6d6984-cf5d-4d29-a312-a8b9b395561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.790794241769082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first = sum(first_a)/len(first_a)\n",
    "Q_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a943deb-cd88-45d0-a730-7c7cc6bf21e4",
   "metadata": {},
   "source": [
    "This is a simple way of incrementing steps and adding reward values. To (manually) visualise the convergence run the above to blocks of code a few times until ```Q_first``` srarts decreasing.\n",
    "\n",
    "**Note**: To make our code cleaner and easier to read we can add the incremental part in a function. This function will do exactly what the last formula (with the updating rule does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a36b061-9d7c-4797-b5ca-72dc329da252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_rule(aval, m, n):\n",
    "    \n",
    "    # aval = (previous) average value\n",
    "    # m = np array with means of k-arms\n",
    "    # n = step size\n",
    "    \n",
    "    n = n + 1 # increment step size by 1\n",
    "    rewards = np.zeros(m.shape)\n",
    "    \n",
    "    for j in range(s):\n",
    "        a = np.array([[j]])\n",
    "        r = eval_env.step(a)\n",
    "        rewards[0,j] = r\n",
    "        \n",
    "    temp = rewards[0,0]\n",
    "    upd_aver = aval + 1 / n * (temp - aval)\n",
    "    return upd_aver, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d99884-fd54-42ca-9788-a5b69e5f32bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.736925940214931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first, n = incremental_rule(Q_first, m, n)   \n",
    "Q_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b6545-667d-4e04-aba5-d3603e8d2cdf",
   "metadata": {},
   "source": [
    "## Exploitation - The Greedy Agent \n",
    "\n",
    "As mentioned earlier, there are two startegies for choosing the best action each time. The agent can either exploit or explore actions and their rewards. \n",
    "\n",
    "I will focus on exploitation for now. In k-armed badint problems the optimal behaviour is that the agent chooses the action that gives the best reward in the long run. The agent needs to interact with the environment in order to learn. However, there might be times that the agent is **greedy**, that is, he does not choose to explore all actions and reward values, but chooses the action that gives the current maximum expected value. Think of this behaviour like going to a nice restaurant and choosing one specific meal every single time, because you ordered it once and you liked it. But this kind of behaviour does not let you explore other choises. \n",
    "\n",
    "This is exploitation and when an agent chooses greedy actions we use the ```argmax()``` function of the expected values. In cases of exploitation, if more than one action is considered best, then the agent chooses one of these best choices randomly. This refers to breaking ties between multiple max elements so that the first element only is returned. We are goint to use the ```np.argmax()``` function to do that. Let's built **our** ```argmax()``` function. \n",
    "\n",
    "We will generate a template with all the values in the input matrix that are equal to the max value. We will then generate some noise and lastly we will multiply the noise array with the template and use the np.argmax method to get the indices of the maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b8e9a79-3bd8-4842-9959-763f00d3426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(mat):\n",
    "    \n",
    "    '''\n",
    "    Input: N x k matrix with reward estimates \n",
    "    Output: array with index of the item(s) with the (first) highest value per row in the matrix \n",
    "    \n",
    "    Uses the np.argmax() function with tie breaking between multiple max elements.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # first create the template:\n",
    "    template = mat == mat.max(axis=1)[:, None]\n",
    "    \n",
    "    # generate some random noise:\n",
    "    noise = 1e-6*np.random.random(mat.shape)\n",
    "    \n",
    "    # multiply the noise with the template values:\n",
    "    temp_n = noise * template\n",
    "    \n",
    "    # get the argmax of the template using the np.argmax() function. This will return only the \n",
    "    # first max element in case there are more than 1\n",
    "    outmat = np.argmax(temp_n, axis=1)\n",
    "    \n",
    "    return outmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd7127-24f3-498e-8740-5fc22bded992",
   "metadata": {},
   "source": [
    "The next step requires creating our ```GreedyAgent``` class. The agent will be learn to interact with the Env that we created earlier. The agent takes an action based on based on which action returns the highest reward but it can also learn from the action that it chose by updating the values for each action using the incremental averaging rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "117e3aea-d6c3-4ca8-b016-29e841ec9c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    \n",
    "    def __init__(self, e, b):\n",
    "        \n",
    "        '''\n",
    "        The Greedy Agent takes the number of experiments and bandits as input and updates the estimates of \n",
    "        the reward incrementally whenever the agent interacts with the environment\n",
    "         \n",
    "         INPUTS:\n",
    "         e = number of experiments\n",
    "         b = number of bandits \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        estimates = np.zeros((e, b))\n",
    "        \n",
    "        self.nbandits = b\n",
    "        self.nexperiments = e\n",
    "        self.estimates = estimates.astype(np.float64) \n",
    "        self.a_count = np.zeros(self.estimates.shape)\n",
    "        \n",
    "    def greedy_action(self):\n",
    "        \n",
    "        # we'll just use the argmax function:\n",
    "        max_action = argmax(self.estimates)\n",
    "        \n",
    "        # if an action in self.a_count is selected (for having the highest value), add the value of 1\n",
    "        self.a_count[np.arange(self.nexperiments), max_action] += 1\n",
    "        ac = self.a_count\n",
    "        estm = self.estimates\n",
    "        \n",
    "        return max_action, ac, estm\n",
    "    \n",
    "    def estimate_upd(self, r, a):\n",
    "    \n",
    "        '''\n",
    "        Input arguments:\n",
    "        r = reward,\n",
    "        a = (max) actions\n",
    "    \n",
    "        Output: updated reward estimates \n",
    "    \n",
    "        We use this function to update the reward estimates incrementally '''\n",
    "    \n",
    "        n = self.a_count[np.arange(self.nexperiments), a]\n",
    "    \n",
    "        # difference between received rewards and reward estimates \n",
    "        e = r - self.estimates[np.arange(self.nexperiments), a]\n",
    "    \n",
    "        # update the reward diff incrementally\n",
    "        self.estimates[np.arange(self.nexperiments), a] += (1/n) * e\n",
    "        act_estimates = self.estimates\n",
    "    \n",
    "        return act_estimates   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc0e71-f47f-4d3d-94c2-bd04a2800868",
   "metadata": {},
   "source": [
    "### Environment - Agent interaction\n",
    "So, how do we expect our agent to behave? At first it wont have much knowledge of the environemnt, so it will expect that each action is the same. After interacting with the environment a few times it should start learning and differentiating between actions and their rewards. However, it may also keep choosing the action that gives the highest reward (over, and over) until the action stops providing high reward.\n",
    "\n",
    "**Is the second a good strategy?**\n",
    "\n",
    "Now that we created the environment and the agent let us test how it behaves. We will see how the agent estimates the values for each action as well as the real values provided by the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f21465f-4600-4cc9-9a51-b63d0746c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 2\n",
    "n_bandits = 4\n",
    "n_steps = 10\n",
    "m = np.random.normal(size=(exp, n_bandits))\n",
    "sd = np.ones((exp, n_bandits))\n",
    "\n",
    "# Create a new env and init the agent \n",
    "env = BanditsEnv(m, sd, exp)\n",
    "agent = GreedyAgent(exp, n_bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d6b018e-f512-4c74-8509-4d6a5d0db72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have our environment and agent, it's time for the interaction between them. we'll add a loop \n",
    "# and the interaction will take place n_steps times:\n",
    "actual_estimates = []\n",
    "for i in range(n_steps):\n",
    "    max_action, action_count, estim = agent.greedy_action() # max action chosen by the agent\n",
    "    reward = env.step(max_action) # get reward for that action\n",
    "    temp = agent.estimate_upd(reward, max_action)\n",
    "    actual_estimates.append(temp)\n",
    "    \n",
    "# actual_estimates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0bcc0-60a3-41d7-8af8-8e5bf7dcaa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc740-13b0-4ace-b345-17b861725ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cdd01-09f2-44d8-a30b-7f4feb7cfbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321bebf-f0c6-472c-ac44-1ac9b257556f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d2449-f9ad-446c-9fcf-9ce772965f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa66eba-8076-4f4b-bf16-4b64b79a0bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90fd08-8030-4cd6-ab06-a4103198a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the interaction between the environment and the agent \n",
    "fig, axs = plt.subplots(1, n_exp, figsize=(10,4))\n",
    "x_pos = np.arange(n_bandits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
